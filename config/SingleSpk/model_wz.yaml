transformer:
  encoder_layer: 5
  encoder_head: 2
  encoder_hidden: 256
  decoder_layer: 6
  decoder_head: 2
  decoder_hidden: 256
  conv_filter_size: 1024
  conv_kernel_size: [9, 1]
  encoder_dropout: 0.1
  decoder_dropout: 0.1
  encoder_kernel_sizes: [3, 5, 5, 7, 9]
  decoder_kernel_sizes: [5, 5, 7, 7, 9, 9]

variance_predictor:
  filter_size: [256, 384]
  kernel_size: [3, 3]
  dropout: 0.5
  dropout_on_emb: 0.1 # dropout applied to pitch&energy embs before adding to decoder input, doesn't affect predictors directly


# If you are using adversarial training, you can enable bidirectional and use_cbam
# Otherwise, on simple regression objective, it will overfit.
duration_predictor:
  type: "lstm" # support tcn (TCN-Attention) or lstm (Conv+Decoder+LSTM)
  bidirectional: true # both: bidirectional
  att_dropout: 0.5 # only applies to TCN duration pred, dropout applied to attention
  tcn_channels: [256, 256] # only applies to TCN duration pred
  tcn_heads: [0, 2] #tcn: attention heads for each layer. you can use 0 to disable multi-head and use squeeze-excite
  tcn_kernel_sizes: [3, 3] #tcn: kernel sizes for each layer
  backwards_tcn_channels: [256, 256]
  backwards_heads: [0, 0]
  backwards_kernel_sizes: [3, 3]
  conv_depth: 2 # lstm: depth of pre-convs
  kernel_size: 3 # lstm: conv kernel size
  filter_size: 256 # lstm: hidden size
  dropout: 0.5 # both: dropout
  use_cbam: false #LSTM: use CBAM

# The discriminator config is very flexible
# You can define one or multiple discriminators, with diff conv sizes and S4 layers (or lack thereof) for each.
discriminator:
  kernel_sizes: [ [3, 3, 5, 7, 9, 11] ] # List of lists detailing kernel sizes for each discriminator
  # kernel_sizes: [[3, 3, 5, 5], [5, 7, 9, 11]] builds 2 discriminators, one with [3, 3, 5, 5] convs, and the other [5, 7, 9, 11].

  ssm_depth: [6] # Num of S4 layers for each discriminator. You can set to 0 for no S4 layers.
  ssm_dropout: 0.2 # Dropout on S4 layers (if there are any)
  conv_dropout: 0.35 # Dropout on Conv layers
  hidden_size: 1024 # Hidden dim.
  norm: "layer" # accepts "layer" or "spectral"


variance_embedding:
  pitch_quantization: "linear" # support 'linear' or 'log', 'log' is allowed only if the pitch values are not normalized during preprocessing
  energy_quantization: "linear" # support 'linear' or 'log', 'log' is allowed only if the energy values are not normalized during preprocessing
  n_bins: 256

# gst:
#   use_gst: False
#   conv_filters: [32, 32, 64, 64, 128, 128]
#   gru_hidden: 128
#   token_size: 128
#   n_style_token: 10
#   attn_head: 4

multi_speaker: False
max_seq_len: 1000
emotion_size: 256
speaker_channels: 0
aligner: "mas"
mas_channels: 256
em_enc_sizes: [312, 128, 64, 16]

vocoder:
  model: "iSTFTNet" # support 'HiFi-GAN', 'MelGAN'
  speaker: "LJSpeech" # support  'LJSpeech', 'universal'
