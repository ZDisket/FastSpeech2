transformer:
  encoder_layer: 4
  encoder_head: 2
  encoder_hidden: 256
  decoder_layer: 6
  decoder_head: 2
  decoder_hidden: 256
  conv_filter_size: 1024
  conv_kernel_size: [9, 1]
  encoder_dropout: 0.1
  decoder_dropout: 0.15

variance_predictor:
  filter_size: 256
  kernel_size: 3
  dropout: 0.1
  dropout_on_emb: 0.3 # dropout applied to pitch&energy embs before adding to decoder input, doesn't affect predictors directly

duration_predictor:
  type: "tcn" # support tcn or lstm
  att_dropout: 0.2 # only applies to TCN duration pred, dropout applied to attention
  tcn_channels: [256, 384, 512] # only applies to TCN duration pred
  decoder_depth: 2 # lstm: depth of auxiliary decoder
  conv_depth: 2 # lstm: depth of pre-convs
  kernel_size: 3 # both: conv kernel size
  heads: 2 # both: attention heads
  filter_size: 256 # lstm: hidden size
  dropout: 0.1 # both: dropout
  bidirectional: true # both: bidirectional


variance_embedding:
  pitch_quantization: "linear" # support 'linear' or 'log', 'log' is allowed only if the pitch values are not normalized during preprocessing
  energy_quantization: "linear" # support 'linear' or 'log', 'log' is allowed only if the energy values are not normalized during preprocessing
  n_bins: 256

# gst:
#   use_gst: False
#   conv_filters: [32, 32, 64, 64, 128, 128]
#   gru_hidden: 128
#   token_size: 128
#   n_style_token: 10
#   attn_head: 4

multi_speaker: False
max_seq_len: 1000

vocoder:
  model: "iSTFTNet" # support 'HiFi-GAN', 'MelGAN'
  speaker: "LJSpeech" # support  'LJSpeech', 'universal'
